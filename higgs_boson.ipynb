{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append('./scripts')\n",
    "\n",
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y_train, X_train, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)\n",
    "\n",
    "feature_list = [\"DER_mass_MMC\", \"DER_mass_transverse_met_lep\", \"DER_mass_vis\", \"DER_pt_h\", \"DER_deltaeta_jet_jet\", \"DER_mass_jet_jet\",\n",
    "                \"DER_prodeta_jet_jet\", \"DER_deltar_tau_lep\", \"DER_pt_tot\", \"DER_sum_pt\", \"DER_pt_ratio_lep_tau\", \"DER_met_phi_centrality\",\n",
    "                \"DER_lep_eta_centrality\", \"PRI_tau_pt\", \"PRI_tau_eta\", \"PRI_tau_phi\", \"PRI_lep_pt\", \"PRI_lep_eta\", \"PRI_lep_phi\",\n",
    "                \"PRI_met\", \"PRI_met_phi\", \"PRI_met_sumet\", \"PRI_jet_num\", \"PRI_jet_leading_pt\", \"PRI_jet_leading_eta\",\n",
    "                \"PRI_jet_leading_phi\", \"PRI_jet_subleading_pt\", \"PRI_jet_subleading_eta\", \"PRI_jet_subleading_phi\", \"PRI_jet_all_pt\"]\n",
    "feature_ids = {feature:i for i, feature in enumerate(feature_list)}\n",
    "\n",
    "# TODO: remove some features in advance ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return np.sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-20)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_custom_features(X, custom_feature_ids):\n",
    "    X_cleaned = np.delete(X, custom_feature_ids, axis=1)\n",
    "    return X_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tiny_features(X, threshold=1):\n",
    "    d = X.shape[1]\n",
    "    remove_features = []\n",
    "    \n",
    "    for i in range(d):\n",
    "        unique_values = np.unique(X[:, i])\n",
    "        if(len(unique_values) <=threshold):\n",
    "            remove_features.append(i)\n",
    "\n",
    "    X_cleaned = np.delete(X, remove_features, axis=1)\n",
    "    return X_cleaned, remove_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(X, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    \n",
    "    d = X.shape[1]\n",
    "    X_poly = []\n",
    "    for i in range(0, degree + 1):\n",
    "        X_poly.append(X ** (i + 1))\n",
    "        \n",
    "    X_poly.append(np.ones((X.shape[0], 1)))\n",
    "    X_poly = np.concatenate(X_poly, axis=1)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    IT SEEMS THAT CURRENTLY SIN/COS FEATURES HARM PERFORMANCE\n",
    "    \n",
    "    # add sin and cos to basis\n",
    "    X_sin = np.sin(X)\n",
    "    X_cos = np.cos(X)\n",
    "    X_poly = np.concatenate((X_poly, X_sin, X_cos), axis=1)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # cross terms of second degree\n",
    "    X_cross = []\n",
    "    for i in range(d):\n",
    "        for j in range(d):\n",
    "            if i != j:\n",
    "                X_cross.append((X[:, i] * X[:, j]).reshape(-1, 1))\n",
    "                \n",
    "    X_cross = np.concatenate(X_cross, axis=1)\n",
    "    X_final = np.concatenate((X_poly, X_cross), axis=1)    \n",
    "    return X_final\n",
    "    \"\"\"\n",
    "    \n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(y, X, k_indices, k, degree, gamma, lambda_, max_iters, batch_size):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    \n",
    "    X, _ = remove_tiny_features(X)\n",
    "    \n",
    "    X_test = X[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    X_train = np.vstack([X[k_indices[i]] for i in range(k_indices.shape[0]) if not i == k])\n",
    "    y_train = np.hstack([y[k_indices[i]] for i in range(k_indices.shape[0]) if not i == k])\n",
    "    \n",
    "    # remove outliers\n",
    "    # impute values when they are missing (value -999 I think) (e.g. by median)\n",
    "    \n",
    "    X_test = build_poly(X_test, degree)\n",
    "    X_train = build_poly(X_train, degree)\n",
    "     \n",
    "    X_train = standardize(X_train)\n",
    "    X_test = standardize(X_test)\n",
    "    \n",
    "    w0 = np.zeros(X_train.shape[1])\n",
    "    w, loss = reg_logistic_regression(y=y_train, tx=X_train, lambda_=lambda_, initial_w=w0, max_iters=max_iters, gamma=gamma, batch_size=batch_size)\n",
    "    \n",
    "    y_train_pred = predict_labels(w, X_train)\n",
    "    y_test_pred = predict_labels(w, X_test)\n",
    "    \n",
    "    acc_train = accuracy(y_train_pred, y_train)\n",
    "    acc_test = accuracy(y_test_pred, y_test)\n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_feature(y, X, ids, feature_id, train=True):\n",
    "    unique_values = np.unique(X[:, feature_id])\n",
    "    X_new = np.delete(X, feature_id, axis=1)\n",
    "    splits = {}\n",
    "    for value in unique_values:\n",
    "        X_cur = X_new[np.where(X[:, feature_id] == value)]\n",
    "        y_cur = None\n",
    "        if(train):\n",
    "            y_cur = y[np.where(X[:, feature_id] == value)]\n",
    "        else:\n",
    "            y_cur = ids[np.where(X[:, feature_id] == value)]\n",
    "        ids_cur = ids[np.where(X[:, feature_id] == value)]\n",
    "        splits[value] = (X_cur, y_cur, ids_cur)\n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JET NUMBER 0:\n",
      "Degree:  1, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.6949 +- 0.0054, Test: 0.6843 +- 0.0361\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:  0.001: Train: 0.6955 +- 0.0073, Test: 0.6939 +- 0.0395\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:   0.01: Train: 0.6966 +- 0.0057, Test: 0.6919 +- 0.0424\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:    0.1: Train: 0.6957 +- 0.0070, Test: 0.6929 +- 0.0400\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:    1.0: Train: 0.6945 +- 0.0040, Test: 0.6914 +- 0.0393\n",
      "Degree:  1, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7412 +- 0.0069, Test: 0.7348 +- 0.0440\n",
      "Degree:  1, Learning rate:  0.001, Lambda:  0.001: Train: 0.7401 +- 0.0071, Test: 0.7298 +- 0.0439\n",
      "Degree:  1, Learning rate:  0.001, Lambda:   0.01: Train: 0.7418 +- 0.0074, Test: 0.7318 +- 0.0422\n",
      "Degree:  1, Learning rate:  0.001, Lambda:    0.1: Train: 0.7423 +- 0.0072, Test: 0.7298 +- 0.0462\n",
      "Degree:  1, Learning rate:  0.001, Lambda:    1.0: Train: 0.7360 +- 0.0078, Test: 0.7338 +- 0.0406\n",
      "Degree:  1, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7330 +- 0.0135, Test: 0.7177 +- 0.0355\n",
      "Degree:  1, Learning rate:   0.01, Lambda:  0.001: Train: 0.7247 +- 0.0285, Test: 0.7187 +- 0.0293\n",
      "Degree:  1, Learning rate:   0.01, Lambda:   0.01: Train: 0.7253 +- 0.0243, Test: 0.7101 +- 0.0387\n",
      "Degree:  1, Learning rate:   0.01, Lambda:    0.1: Train: 0.7316 +- 0.0234, Test: 0.7152 +- 0.0425\n",
      "Degree:  1, Learning rate:   0.01, Lambda:    1.0: Train: 0.7155 +- 0.0157, Test: 0.7106 +- 0.0288\n",
      "Degree:  1, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6583 +- 0.0416, Test: 0.6495 +- 0.0341\n",
      "Degree:  1, Learning rate:    0.1, Lambda:  0.001: Train: 0.6116 +- 0.0627, Test: 0.6141 +- 0.0493\n",
      "Degree:  1, Learning rate:    0.1, Lambda:   0.01: Train: 0.6236 +- 0.0793, Test: 0.6126 +- 0.0921\n",
      "Degree:  1, Learning rate:    0.1, Lambda:    0.1: Train: 0.6371 +- 0.0589, Test: 0.6328 +- 0.0779\n",
      "Degree:  1, Learning rate:    0.1, Lambda:    1.0: Train: 0.5802 +- 0.0727, Test: 0.5475 +- 0.0910\n",
      "Degree:  2, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7007 +- 0.0083, Test: 0.7030 +- 0.0387\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7008 +- 0.0062, Test: 0.6965 +- 0.0332\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:   0.01: Train: 0.6999 +- 0.0061, Test: 0.6970 +- 0.0395\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7005 +- 0.0057, Test: 0.7000 +- 0.0364\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7014 +- 0.0069, Test: 0.6985 +- 0.0401\n",
      "Degree:  2, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7456 +- 0.0047, Test: 0.7298 +- 0.0334\n",
      "Degree:  2, Learning rate:  0.001, Lambda:  0.001: Train: 0.7448 +- 0.0090, Test: 0.7364 +- 0.0401\n",
      "Degree:  2, Learning rate:  0.001, Lambda:   0.01: Train: 0.7426 +- 0.0077, Test: 0.7278 +- 0.0432\n",
      "Degree:  2, Learning rate:  0.001, Lambda:    0.1: Train: 0.7459 +- 0.0088, Test: 0.7364 +- 0.0455\n",
      "Degree:  2, Learning rate:  0.001, Lambda:    1.0: Train: 0.7410 +- 0.0087, Test: 0.7343 +- 0.0432\n",
      "Degree:  2, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7195 +- 0.0234, Test: 0.7035 +- 0.0264\n",
      "Degree:  2, Learning rate:   0.01, Lambda:  0.001: Train: 0.7154 +- 0.0362, Test: 0.6904 +- 0.0392\n",
      "Degree:  2, Learning rate:   0.01, Lambda:   0.01: Train: 0.7264 +- 0.0267, Test: 0.7061 +- 0.0417\n",
      "Degree:  2, Learning rate:   0.01, Lambda:    0.1: Train: 0.7251 +- 0.0267, Test: 0.7232 +- 0.0399\n",
      "Degree:  2, Learning rate:   0.01, Lambda:    1.0: Train: 0.7005 +- 0.0087, Test: 0.6919 +- 0.0218\n",
      "Degree:  2, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6583 +- 0.0535, Test: 0.6424 +- 0.0659\n",
      "Degree:  2, Learning rate:    0.1, Lambda:  0.001: Train: 0.6109 +- 0.0515, Test: 0.5995 +- 0.0625\n",
      "Degree:  2, Learning rate:    0.1, Lambda:   0.01: Train: 0.6298 +- 0.0820, Test: 0.6232 +- 0.0866\n",
      "Degree:  2, Learning rate:    0.1, Lambda:    0.1: Train: 0.6157 +- 0.0454, Test: 0.5828 +- 0.0513\n",
      "Degree:  2, Learning rate:    0.1, Lambda:    1.0: Train: 0.5772 +- 0.0579, Test: 0.5702 +- 0.0637\n",
      "Degree:  3, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7047 +- 0.0073, Test: 0.7040 +- 0.0381\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7057 +- 0.0052, Test: 0.7096 +- 0.0349\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7060 +- 0.0069, Test: 0.7061 +- 0.0363\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7056 +- 0.0070, Test: 0.7126 +- 0.0335\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7062 +- 0.0071, Test: 0.7116 +- 0.0341\n",
      "Degree:  3, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7476 +- 0.0082, Test: 0.7343 +- 0.0304\n",
      "Degree:  3, Learning rate:  0.001, Lambda:  0.001: Train: 0.7442 +- 0.0085, Test: 0.7374 +- 0.0413\n",
      "Degree:  3, Learning rate:  0.001, Lambda:   0.01: Train: 0.7479 +- 0.0085, Test: 0.7364 +- 0.0399\n",
      "Degree:  3, Learning rate:  0.001, Lambda:    0.1: Train: 0.7393 +- 0.0099, Test: 0.7333 +- 0.0403\n",
      "Degree:  3, Learning rate:  0.001, Lambda:    1.0: Train: 0.7402 +- 0.0085, Test: 0.7293 +- 0.0371\n",
      "Degree:  3, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7242 +- 0.0228, Test: 0.7076 +- 0.0359\n",
      "Degree:  3, Learning rate:   0.01, Lambda:  0.001: Train: 0.7093 +- 0.0185, Test: 0.6965 +- 0.0284\n",
      "Degree:  3, Learning rate:   0.01, Lambda:   0.01: Train: 0.7075 +- 0.0239, Test: 0.6818 +- 0.0234\n",
      "Degree:  3, Learning rate:   0.01, Lambda:    0.1: Train: 0.7038 +- 0.0277, Test: 0.6788 +- 0.0524\n",
      "Degree:  3, Learning rate:   0.01, Lambda:    1.0: Train: 0.6674 +- 0.0690, Test: 0.6747 +- 0.0768\n",
      "Degree:  3, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6396 +- 0.0755, Test: 0.6207 +- 0.0811\n",
      "Degree:  3, Learning rate:    0.1, Lambda:  0.001: Train: 0.6648 +- 0.0429, Test: 0.6677 +- 0.0535\n",
      "Degree:  3, Learning rate:    0.1, Lambda:   0.01: Train: 0.6337 +- 0.0577, Test: 0.6333 +- 0.0799\n",
      "Degree:  3, Learning rate:    0.1, Lambda:    0.1: Train: 0.6429 +- 0.0612, Test: 0.6283 +- 0.0477\n",
      "Degree:  3, Learning rate:    0.1, Lambda:    1.0: Train: 0.5748 +- 0.0867, Test: 0.5480 +- 0.0766\n",
      "Degree:  4, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7052 +- 0.0073, Test: 0.7101 +- 0.0340\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7086 +- 0.0077, Test: 0.7121 +- 0.0399\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7058 +- 0.0067, Test: 0.7121 +- 0.0355\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7081 +- 0.0087, Test: 0.7101 +- 0.0341\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7048 +- 0.0064, Test: 0.7136 +- 0.0363\n",
      "Degree:  4, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7433 +- 0.0076, Test: 0.7359 +- 0.0335\n",
      "Degree:  4, Learning rate:  0.001, Lambda:  0.001: Train: 0.7405 +- 0.0083, Test: 0.7379 +- 0.0318\n",
      "Degree:  4, Learning rate:  0.001, Lambda:   0.01: Train: 0.7393 +- 0.0101, Test: 0.7318 +- 0.0290\n",
      "Degree:  4, Learning rate:  0.001, Lambda:    0.1: Train: 0.7423 +- 0.0098, Test: 0.7313 +- 0.0382\n",
      "Degree:  4, Learning rate:  0.001, Lambda:    1.0: Train: 0.7363 +- 0.0138, Test: 0.7288 +- 0.0373\n",
      "Degree:  4, Learning rate:   0.01, Lambda: 0.0001: Train: 0.6847 +- 0.0406, Test: 0.6727 +- 0.0604\n",
      "Degree:  4, Learning rate:   0.01, Lambda:  0.001: Train: 0.6909 +- 0.0408, Test: 0.6510 +- 0.0425\n",
      "Degree:  4, Learning rate:   0.01, Lambda:   0.01: Train: 0.7098 +- 0.0280, Test: 0.6924 +- 0.0603\n",
      "Degree:  4, Learning rate:   0.01, Lambda:    0.1: Train: 0.7085 +- 0.0332, Test: 0.6924 +- 0.0479\n",
      "Degree:  4, Learning rate:   0.01, Lambda:    1.0: Train: 0.6782 +- 0.0494, Test: 0.6606 +- 0.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/postuvan/EPFL/ML_project1/./scripts/implementations.py:77: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree:  4, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6485 +- 0.0584, Test: 0.6399 +- 0.0605\n",
      "Degree:  4, Learning rate:    0.1, Lambda:  0.001: Train: 0.6606 +- 0.0585, Test: 0.6631 +- 0.0677\n",
      "Degree:  4, Learning rate:    0.1, Lambda:   0.01: Train: 0.6383 +- 0.0492, Test: 0.6369 +- 0.0630\n",
      "Degree:  4, Learning rate:    0.1, Lambda:    0.1: Train: 0.6192 +- 0.0605, Test: 0.6157 +- 0.0654\n",
      "Degree:  4, Learning rate:    0.1, Lambda:    1.0: Train: 0.5709 +- 0.0625, Test: 0.5934 +- 0.0658\n",
      "Degree:  5, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7054 +- 0.0090, Test: 0.7096 +- 0.0394\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7042 +- 0.0077, Test: 0.7076 +- 0.0325\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7052 +- 0.0093, Test: 0.7131 +- 0.0337\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7080 +- 0.0095, Test: 0.7121 +- 0.0320\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7068 +- 0.0081, Test: 0.7111 +- 0.0355\n",
      "Degree:  5, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7432 +- 0.0112, Test: 0.7328 +- 0.0405\n",
      "Degree:  5, Learning rate:  0.001, Lambda:  0.001: Train: 0.7435 +- 0.0076, Test: 0.7354 +- 0.0345\n",
      "Degree:  5, Learning rate:  0.001, Lambda:   0.01: Train: 0.7419 +- 0.0078, Test: 0.7328 +- 0.0395\n",
      "Degree:  5, Learning rate:  0.001, Lambda:    0.1: Train: 0.7400 +- 0.0096, Test: 0.7369 +- 0.0373\n",
      "Degree:  5, Learning rate:  0.001, Lambda:    1.0: Train: 0.7343 +- 0.0071, Test: 0.7283 +- 0.0417\n",
      "Degree:  5, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7204 +- 0.0252, Test: 0.7237 +- 0.0422\n",
      "Degree:  5, Learning rate:   0.01, Lambda:  0.001: Train: 0.7052 +- 0.0350, Test: 0.6970 +- 0.0578\n",
      "Degree:  5, Learning rate:   0.01, Lambda:   0.01: Train: 0.7036 +- 0.0418, Test: 0.6742 +- 0.0500\n",
      "Degree:  5, Learning rate:   0.01, Lambda:    0.1: Train: 0.7108 +- 0.0297, Test: 0.6995 +- 0.0541\n",
      "Degree:  5, Learning rate:   0.01, Lambda:    1.0: Train: 0.6627 +- 0.0645, Test: 0.6646 +- 0.0674\n",
      "Degree:  5, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6089 +- 0.0654, Test: 0.5975 +- 0.0921\n",
      "Degree:  5, Learning rate:    0.1, Lambda:  0.001: Train: 0.6519 +- 0.0291, Test: 0.6535 +- 0.0545\n",
      "Degree:  5, Learning rate:    0.1, Lambda:   0.01: Train: 0.5838 +- 0.0648, Test: 0.5919 +- 0.0802\n",
      "Degree:  5, Learning rate:    0.1, Lambda:    0.1: Train: 0.6164 +- 0.0524, Test: 0.6313 +- 0.0885\n",
      "Degree:  5, Learning rate:    0.1, Lambda:    1.0: Train: 0.5396 +- 0.0679, Test: 0.5384 +- 0.0902\n",
      "JET NUMBER 1:\n",
      "Degree:  1, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7025 +- 0.0040, Test: 0.6968 +- 0.0257\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7002 +- 0.0052, Test: 0.6968 +- 0.0279\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:   0.01: Train: 0.6994 +- 0.0046, Test: 0.6942 +- 0.0264\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7009 +- 0.0046, Test: 0.6942 +- 0.0251\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7004 +- 0.0047, Test: 0.6948 +- 0.0298\n",
      "Degree:  1, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7125 +- 0.0071, Test: 0.6987 +- 0.0299\n",
      "Degree:  1, Learning rate:  0.001, Lambda:  0.001: Train: 0.7100 +- 0.0069, Test: 0.6903 +- 0.0334\n",
      "Degree:  1, Learning rate:  0.001, Lambda:   0.01: Train: 0.7128 +- 0.0065, Test: 0.6955 +- 0.0308\n",
      "Degree:  1, Learning rate:  0.001, Lambda:    0.1: Train: 0.7131 +- 0.0030, Test: 0.6948 +- 0.0342\n",
      "Degree:  1, Learning rate:  0.001, Lambda:    1.0: Train: 0.7104 +- 0.0069, Test: 0.6994 +- 0.0288\n",
      "Degree:  1, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7165 +- 0.0140, Test: 0.6974 +- 0.0393\n",
      "Degree:  1, Learning rate:   0.01, Lambda:  0.001: Train: 0.7155 +- 0.0142, Test: 0.6955 +- 0.0412\n",
      "Degree:  1, Learning rate:   0.01, Lambda:   0.01: Train: 0.7142 +- 0.0157, Test: 0.6831 +- 0.0396\n",
      "Degree:  1, Learning rate:   0.01, Lambda:    0.1: Train: 0.7097 +- 0.0221, Test: 0.6987 +- 0.0489\n",
      "Degree:  1, Learning rate:   0.01, Lambda:    1.0: Train: 0.7030 +- 0.0206, Test: 0.6818 +- 0.0373\n",
      "Degree:  1, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6688 +- 0.0221, Test: 0.6688 +- 0.0315\n",
      "Degree:  1, Learning rate:    0.1, Lambda:  0.001: Train: 0.6322 +- 0.0658, Test: 0.6071 +- 0.0658\n",
      "Degree:  1, Learning rate:    0.1, Lambda:   0.01: Train: 0.6695 +- 0.0349, Test: 0.6519 +- 0.0359\n",
      "Degree:  1, Learning rate:    0.1, Lambda:    0.1: Train: 0.6145 +- 0.0549, Test: 0.6097 +- 0.0727\n",
      "Degree:  1, Learning rate:    0.1, Lambda:    1.0: Train: 0.6160 +- 0.0745, Test: 0.6175 +- 0.0636\n",
      "Degree:  2, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.6996 +- 0.0020, Test: 0.6870 +- 0.0267\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7006 +- 0.0029, Test: 0.6935 +- 0.0292\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7007 +- 0.0027, Test: 0.7000 +- 0.0300\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:    0.1: Train: 0.6990 +- 0.0037, Test: 0.6968 +- 0.0266\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7024 +- 0.0044, Test: 0.6981 +- 0.0301\n",
      "Degree:  2, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7142 +- 0.0045, Test: 0.6825 +- 0.0350\n",
      "Degree:  2, Learning rate:  0.001, Lambda:  0.001: Train: 0.7117 +- 0.0038, Test: 0.6994 +- 0.0322\n",
      "Degree:  2, Learning rate:  0.001, Lambda:   0.01: Train: 0.7161 +- 0.0074, Test: 0.7006 +- 0.0265\n",
      "Degree:  2, Learning rate:  0.001, Lambda:    0.1: Train: 0.7137 +- 0.0050, Test: 0.6870 +- 0.0378\n",
      "Degree:  2, Learning rate:  0.001, Lambda:    1.0: Train: 0.7142 +- 0.0051, Test: 0.6968 +- 0.0268\n",
      "Degree:  2, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7188 +- 0.0069, Test: 0.6935 +- 0.0333\n",
      "Degree:  2, Learning rate:   0.01, Lambda:  0.001: Train: 0.7130 +- 0.0162, Test: 0.6896 +- 0.0296\n",
      "Degree:  2, Learning rate:   0.01, Lambda:   0.01: Train: 0.7232 +- 0.0126, Test: 0.6968 +- 0.0288\n",
      "Degree:  2, Learning rate:   0.01, Lambda:    0.1: Train: 0.7180 +- 0.0123, Test: 0.6903 +- 0.0436\n",
      "Degree:  2, Learning rate:   0.01, Lambda:    1.0: Train: 0.7091 +- 0.0094, Test: 0.6948 +- 0.0396\n",
      "Degree:  2, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6533 +- 0.0562, Test: 0.6104 +- 0.0796\n",
      "Degree:  2, Learning rate:    0.1, Lambda:  0.001: Train: 0.6490 +- 0.0664, Test: 0.6208 +- 0.0744\n",
      "Degree:  2, Learning rate:    0.1, Lambda:   0.01: Train: 0.6421 +- 0.0516, Test: 0.6253 +- 0.0529\n",
      "Degree:  2, Learning rate:    0.1, Lambda:    0.1: Train: 0.6108 +- 0.0773, Test: 0.5935 +- 0.0997\n",
      "Degree:  2, Learning rate:    0.1, Lambda:    1.0: Train: 0.6154 +- 0.0622, Test: 0.6188 +- 0.0781\n",
      "Degree:  3, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7035 +- 0.0057, Test: 0.6987 +- 0.0349\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7038 +- 0.0050, Test: 0.6994 +- 0.0341\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7038 +- 0.0062, Test: 0.6916 +- 0.0289\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7055 +- 0.0046, Test: 0.6942 +- 0.0351\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7054 +- 0.0057, Test: 0.6981 +- 0.0334\n",
      "Degree:  3, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7118 +- 0.0092, Test: 0.6818 +- 0.0203\n",
      "Degree:  3, Learning rate:  0.001, Lambda:  0.001: Train: 0.7175 +- 0.0051, Test: 0.6903 +- 0.0250\n",
      "Degree:  3, Learning rate:  0.001, Lambda:   0.01: Train: 0.7128 +- 0.0063, Test: 0.6890 +- 0.0374\n",
      "Degree:  3, Learning rate:  0.001, Lambda:    0.1: Train: 0.7142 +- 0.0060, Test: 0.7000 +- 0.0290\n",
      "Degree:  3, Learning rate:  0.001, Lambda:    1.0: Train: 0.7176 +- 0.0083, Test: 0.6942 +- 0.0308\n",
      "Degree:  3, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7219 +- 0.0145, Test: 0.6948 +- 0.0425\n",
      "Degree:  3, Learning rate:   0.01, Lambda:  0.001: Train: 0.7160 +- 0.0194, Test: 0.6909 +- 0.0481\n",
      "Degree:  3, Learning rate:   0.01, Lambda:   0.01: Train: 0.7275 +- 0.0116, Test: 0.6916 +- 0.0324\n",
      "Degree:  3, Learning rate:   0.01, Lambda:    0.1: Train: 0.7206 +- 0.0107, Test: 0.6942 +- 0.0354\n",
      "Degree:  3, Learning rate:   0.01, Lambda:    1.0: Train: 0.7075 +- 0.0203, Test: 0.6740 +- 0.0432\n",
      "Degree:  3, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6794 +- 0.0264, Test: 0.6545 +- 0.0494\n",
      "Degree:  3, Learning rate:    0.1, Lambda:  0.001: Train: 0.6480 +- 0.0452, Test: 0.6338 +- 0.0495\n",
      "Degree:  3, Learning rate:    0.1, Lambda:   0.01: Train: 0.6685 +- 0.0301, Test: 0.6331 +- 0.0551\n",
      "Degree:  3, Learning rate:    0.1, Lambda:    0.1: Train: 0.6714 +- 0.0427, Test: 0.6578 +- 0.0520\n",
      "Degree:  3, Learning rate:    0.1, Lambda:    1.0: Train: 0.6096 +- 0.0743, Test: 0.5838 +- 0.0864\n",
      "Degree:  4, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7064 +- 0.0052, Test: 0.6955 +- 0.0321\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7034 +- 0.0036, Test: 0.6961 +- 0.0280\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7051 +- 0.0047, Test: 0.6922 +- 0.0292\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7045 +- 0.0038, Test: 0.6955 +- 0.0267\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7044 +- 0.0037, Test: 0.6890 +- 0.0256\n",
      "Degree:  4, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7144 +- 0.0075, Test: 0.6883 +- 0.0265\n",
      "Degree:  4, Learning rate:  0.001, Lambda:  0.001: Train: 0.7159 +- 0.0074, Test: 0.6948 +- 0.0201\n",
      "Degree:  4, Learning rate:  0.001, Lambda:   0.01: Train: 0.7162 +- 0.0043, Test: 0.6987 +- 0.0276\n",
      "Degree:  4, Learning rate:  0.001, Lambda:    0.1: Train: 0.7162 +- 0.0071, Test: 0.6942 +- 0.0268\n",
      "Degree:  4, Learning rate:  0.001, Lambda:    1.0: Train: 0.7152 +- 0.0050, Test: 0.6955 +- 0.0316\n",
      "Degree:  4, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7155 +- 0.0135, Test: 0.6877 +- 0.0313\n",
      "Degree:  4, Learning rate:   0.01, Lambda:  0.001: Train: 0.7112 +- 0.0523, Test: 0.6825 +- 0.0652\n",
      "Degree:  4, Learning rate:   0.01, Lambda:   0.01: Train: 0.7208 +- 0.0074, Test: 0.6968 +- 0.0240\n",
      "Degree:  4, Learning rate:   0.01, Lambda:    0.1: Train: 0.7100 +- 0.0334, Test: 0.6968 +- 0.0414\n",
      "Degree:  4, Learning rate:   0.01, Lambda:    1.0: Train: 0.7016 +- 0.0273, Test: 0.6656 +- 0.0267\n",
      "Degree:  4, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6404 +- 0.0585, Test: 0.6234 +- 0.0705\n",
      "Degree:  4, Learning rate:    0.1, Lambda:  0.001: Train: 0.6370 +- 0.0703, Test: 0.6292 +- 0.0593\n",
      "Degree:  4, Learning rate:    0.1, Lambda:   0.01: Train: 0.6835 +- 0.0371, Test: 0.6468 +- 0.0357\n",
      "Degree:  4, Learning rate:    0.1, Lambda:    0.1: Train: 0.6297 +- 0.0555, Test: 0.6117 +- 0.0494\n",
      "Degree:  4, Learning rate:    0.1, Lambda:    1.0: Train: 0.6251 +- 0.0731, Test: 0.6182 +- 0.0780\n",
      "Degree:  5, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7025 +- 0.0037, Test: 0.6929 +- 0.0276\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7024 +- 0.0042, Test: 0.6916 +- 0.0346\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7027 +- 0.0056, Test: 0.6903 +- 0.0300\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7030 +- 0.0046, Test: 0.6916 +- 0.0289\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7032 +- 0.0034, Test: 0.6935 +- 0.0263\n",
      "Degree:  5, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7186 +- 0.0070, Test: 0.6974 +- 0.0355\n",
      "Degree:  5, Learning rate:  0.001, Lambda:  0.001: Train: 0.7153 +- 0.0074, Test: 0.6786 +- 0.0216\n",
      "Degree:  5, Learning rate:  0.001, Lambda:   0.01: Train: 0.7185 +- 0.0058, Test: 0.7026 +- 0.0340\n",
      "Degree:  5, Learning rate:  0.001, Lambda:    0.1: Train: 0.7198 +- 0.0073, Test: 0.6981 +- 0.0331\n",
      "Degree:  5, Learning rate:  0.001, Lambda:    1.0: Train: 0.7180 +- 0.0081, Test: 0.6935 +- 0.0263\n",
      "Degree:  5, Learning rate:   0.01, Lambda: 0.0001: Train: 0.6989 +- 0.0205, Test: 0.6617 +- 0.0446\n",
      "Degree:  5, Learning rate:   0.01, Lambda:  0.001: Train: 0.7135 +- 0.0168, Test: 0.6786 +- 0.0410\n",
      "Degree:  5, Learning rate:   0.01, Lambda:   0.01: Train: 0.7284 +- 0.0131, Test: 0.6883 +- 0.0356\n",
      "Degree:  5, Learning rate:   0.01, Lambda:    0.1: Train: 0.7295 +- 0.0098, Test: 0.6935 +- 0.0400\n",
      "Degree:  5, Learning rate:   0.01, Lambda:    1.0: Train: 0.7015 +- 0.0151, Test: 0.6714 +- 0.0370\n",
      "Degree:  5, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6717 +- 0.0489, Test: 0.6403 +- 0.0437\n",
      "Degree:  5, Learning rate:    0.1, Lambda:  0.001: Train: 0.6747 +- 0.0362, Test: 0.6305 +- 0.0351\n",
      "Degree:  5, Learning rate:    0.1, Lambda:   0.01: Train: 0.6694 +- 0.0320, Test: 0.6636 +- 0.0432\n",
      "Degree:  5, Learning rate:    0.1, Lambda:    0.1: Train: 0.6308 +- 0.0692, Test: 0.6084 +- 0.0738\n",
      "Degree:  5, Learning rate:    0.1, Lambda:    1.0: Train: 0.6143 +- 0.0497, Test: 0.5792 +- 0.0665\n",
      "JET NUMBER 2:\n",
      "Degree:  1, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7272 +- 0.0049, Test: 0.7144 +- 0.0349\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7266 +- 0.0037, Test: 0.7135 +- 0.0309\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7278 +- 0.0045, Test: 0.7058 +- 0.0350\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7270 +- 0.0036, Test: 0.7135 +- 0.0343\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7275 +- 0.0040, Test: 0.7115 +- 0.0370\n",
      "Degree:  1, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7582 +- 0.0066, Test: 0.7221 +- 0.0343\n",
      "Degree:  1, Learning rate:  0.001, Lambda:  0.001: Train: 0.7610 +- 0.0048, Test: 0.7212 +- 0.0322\n",
      "Degree:  1, Learning rate:  0.001, Lambda:   0.01: Train: 0.7572 +- 0.0072, Test: 0.7250 +- 0.0355\n",
      "Degree:  1, Learning rate:  0.001, Lambda:    0.1: Train: 0.7621 +- 0.0079, Test: 0.7260 +- 0.0347\n",
      "Degree:  1, Learning rate:  0.001, Lambda:    1.0: Train: 0.7540 +- 0.0060, Test: 0.7260 +- 0.0289\n",
      "Degree:  1, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7791 +- 0.0156, Test: 0.7279 +- 0.0394\n",
      "Degree:  1, Learning rate:   0.01, Lambda:  0.001: Train: 0.7804 +- 0.0094, Test: 0.7250 +- 0.0451\n",
      "Degree:  1, Learning rate:   0.01, Lambda:   0.01: Train: 0.7725 +- 0.0150, Test: 0.7202 +- 0.0393\n",
      "Degree:  1, Learning rate:   0.01, Lambda:    0.1: Train: 0.7766 +- 0.0099, Test: 0.7260 +- 0.0360\n",
      "Degree:  1, Learning rate:   0.01, Lambda:    1.0: Train: 0.7421 +- 0.0128, Test: 0.7173 +- 0.0283\n",
      "Degree:  1, Learning rate:    0.1, Lambda: 0.0001: Train: 0.7061 +- 0.0303, Test: 0.6567 +- 0.0514\n",
      "Degree:  1, Learning rate:    0.1, Lambda:  0.001: Train: 0.7169 +- 0.0381, Test: 0.6606 +- 0.0447\n",
      "Degree:  1, Learning rate:    0.1, Lambda:   0.01: Train: 0.7217 +- 0.0507, Test: 0.6837 +- 0.0561\n",
      "Degree:  1, Learning rate:    0.1, Lambda:    0.1: Train: 0.6578 +- 0.0388, Test: 0.6404 +- 0.0460\n",
      "Degree:  1, Learning rate:    0.1, Lambda:    1.0: Train: 0.6240 +- 0.0568, Test: 0.6308 +- 0.0704\n",
      "Degree:  2, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7332 +- 0.0060, Test: 0.7096 +- 0.0432\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7339 +- 0.0043, Test: 0.7067 +- 0.0380\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7333 +- 0.0029, Test: 0.7077 +- 0.0415\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7329 +- 0.0053, Test: 0.7087 +- 0.0410\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7335 +- 0.0052, Test: 0.7115 +- 0.0417\n",
      "Degree:  2, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7678 +- 0.0045, Test: 0.7298 +- 0.0361\n",
      "Degree:  2, Learning rate:  0.001, Lambda:  0.001: Train: 0.7704 +- 0.0097, Test: 0.7202 +- 0.0277\n",
      "Degree:  2, Learning rate:  0.001, Lambda:   0.01: Train: 0.7706 +- 0.0064, Test: 0.7202 +- 0.0308\n",
      "Degree:  2, Learning rate:  0.001, Lambda:    0.1: Train: 0.7655 +- 0.0083, Test: 0.7202 +- 0.0267\n",
      "Degree:  2, Learning rate:  0.001, Lambda:    1.0: Train: 0.7598 +- 0.0069, Test: 0.7192 +- 0.0346\n",
      "Degree:  2, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7824 +- 0.0184, Test: 0.6885 +- 0.0518\n",
      "Degree:  2, Learning rate:   0.01, Lambda:  0.001: Train: 0.7756 +- 0.0221, Test: 0.7048 +- 0.0461\n",
      "Degree:  2, Learning rate:   0.01, Lambda:   0.01: Train: 0.7822 +- 0.0168, Test: 0.6875 +- 0.0579\n",
      "Degree:  2, Learning rate:   0.01, Lambda:    0.1: Train: 0.7713 +- 0.0343, Test: 0.6904 +- 0.0438\n",
      "Degree:  2, Learning rate:   0.01, Lambda:    1.0: Train: 0.7286 +- 0.0200, Test: 0.6904 +- 0.0559\n",
      "Degree:  2, Learning rate:    0.1, Lambda: 0.0001: Train: 0.7487 +- 0.0353, Test: 0.6510 +- 0.0617\n",
      "Degree:  2, Learning rate:    0.1, Lambda:  0.001: Train: 0.7213 +- 0.0497, Test: 0.6558 +- 0.0736\n",
      "Degree:  2, Learning rate:    0.1, Lambda:   0.01: Train: 0.7169 +- 0.0303, Test: 0.6587 +- 0.0454\n",
      "Degree:  2, Learning rate:    0.1, Lambda:    0.1: Train: 0.6890 +- 0.0358, Test: 0.6529 +- 0.0540\n",
      "Degree:  2, Learning rate:    0.1, Lambda:    1.0: Train: 0.6524 +- 0.0357, Test: 0.6462 +- 0.0620\n",
      "Degree:  3, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7332 +- 0.0042, Test: 0.7077 +- 0.0345\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7353 +- 0.0046, Test: 0.7087 +- 0.0385\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7329 +- 0.0046, Test: 0.7125 +- 0.0345\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7338 +- 0.0059, Test: 0.7038 +- 0.0379\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7322 +- 0.0051, Test: 0.7115 +- 0.0333\n",
      "Degree:  3, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7740 +- 0.0083, Test: 0.7221 +- 0.0386\n",
      "Degree:  3, Learning rate:  0.001, Lambda:  0.001: Train: 0.7756 +- 0.0054, Test: 0.7240 +- 0.0392\n",
      "Degree:  3, Learning rate:  0.001, Lambda:   0.01: Train: 0.7733 +- 0.0096, Test: 0.7212 +- 0.0330\n",
      "Degree:  3, Learning rate:  0.001, Lambda:    0.1: Train: 0.7771 +- 0.0084, Test: 0.7163 +- 0.0328\n",
      "Degree:  3, Learning rate:  0.001, Lambda:    1.0: Train: 0.7679 +- 0.0065, Test: 0.7279 +- 0.0370\n",
      "Degree:  3, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7809 +- 0.0143, Test: 0.6962 +- 0.0664\n",
      "Degree:  3, Learning rate:   0.01, Lambda:  0.001: Train: 0.7771 +- 0.0159, Test: 0.6740 +- 0.0627\n",
      "Degree:  3, Learning rate:   0.01, Lambda:   0.01: Train: 0.7821 +- 0.0131, Test: 0.6923 +- 0.0389\n",
      "Degree:  3, Learning rate:   0.01, Lambda:    0.1: Train: 0.7804 +- 0.0118, Test: 0.7183 +- 0.0336\n",
      "Degree:  3, Learning rate:   0.01, Lambda:    1.0: Train: 0.7447 +- 0.0249, Test: 0.6971 +- 0.0449\n",
      "Degree:  3, Learning rate:    0.1, Lambda: 0.0001: Train: 0.7286 +- 0.0300, Test: 0.6654 +- 0.0557\n",
      "Degree:  3, Learning rate:    0.1, Lambda:  0.001: Train: 0.7427 +- 0.0197, Test: 0.6490 +- 0.0388\n",
      "Degree:  3, Learning rate:    0.1, Lambda:   0.01: Train: 0.7267 +- 0.0209, Test: 0.6471 +- 0.0597\n",
      "Degree:  3, Learning rate:    0.1, Lambda:    0.1: Train: 0.6743 +- 0.0369, Test: 0.6510 +- 0.0636\n",
      "Degree:  3, Learning rate:    0.1, Lambda:    1.0: Train: 0.6076 +- 0.0416, Test: 0.5942 +- 0.0671\n",
      "Degree:  4, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7373 +- 0.0048, Test: 0.7096 +- 0.0321\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7378 +- 0.0048, Test: 0.7067 +- 0.0328\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7366 +- 0.0054, Test: 0.7106 +- 0.0287\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7355 +- 0.0041, Test: 0.7048 +- 0.0295\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7364 +- 0.0047, Test: 0.7087 +- 0.0279\n",
      "Degree:  4, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7803 +- 0.0065, Test: 0.7183 +- 0.0279\n",
      "Degree:  4, Learning rate:  0.001, Lambda:  0.001: Train: 0.7769 +- 0.0061, Test: 0.7240 +- 0.0390\n",
      "Degree:  4, Learning rate:  0.001, Lambda:   0.01: Train: 0.7811 +- 0.0061, Test: 0.7212 +- 0.0339\n",
      "Degree:  4, Learning rate:  0.001, Lambda:    0.1: Train: 0.7791 +- 0.0072, Test: 0.7202 +- 0.0404\n",
      "Degree:  4, Learning rate:  0.001, Lambda:    1.0: Train: 0.7738 +- 0.0103, Test: 0.7202 +- 0.0280\n",
      "Degree:  4, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7842 +- 0.0198, Test: 0.6788 +- 0.0548\n",
      "Degree:  4, Learning rate:   0.01, Lambda:  0.001: Train: 0.7889 +- 0.0127, Test: 0.6913 +- 0.0541\n",
      "Degree:  4, Learning rate:   0.01, Lambda:   0.01: Train: 0.7845 +- 0.0121, Test: 0.6788 +- 0.0390\n",
      "Degree:  4, Learning rate:   0.01, Lambda:    0.1: Train: 0.7723 +- 0.0205, Test: 0.6846 +- 0.0533\n",
      "Degree:  4, Learning rate:   0.01, Lambda:    1.0: Train: 0.7446 +- 0.0146, Test: 0.6750 +- 0.0377\n",
      "Degree:  4, Learning rate:    0.1, Lambda: 0.0001: Train: 0.7275 +- 0.0336, Test: 0.6385 +- 0.0605\n",
      "Degree:  4, Learning rate:    0.1, Lambda:  0.001: Train: 0.7124 +- 0.0409, Test: 0.6279 +- 0.0696\n",
      "Degree:  4, Learning rate:    0.1, Lambda:   0.01: Train: 0.7282 +- 0.0189, Test: 0.6587 +- 0.0539\n",
      "Degree:  4, Learning rate:    0.1, Lambda:    0.1: Train: 0.6817 +- 0.0418, Test: 0.6260 +- 0.0533\n",
      "Degree:  4, Learning rate:    0.1, Lambda:    1.0: Train: 0.6467 +- 0.0333, Test: 0.6337 +- 0.0395\n",
      "Degree:  5, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7363 +- 0.0040, Test: 0.7096 +- 0.0231\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7377 +- 0.0065, Test: 0.7087 +- 0.0262\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7358 +- 0.0060, Test: 0.7058 +- 0.0207\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7339 +- 0.0035, Test: 0.7173 +- 0.0203\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7346 +- 0.0066, Test: 0.7115 +- 0.0219\n",
      "Degree:  5, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7807 +- 0.0092, Test: 0.7192 +- 0.0268\n",
      "Degree:  5, Learning rate:  0.001, Lambda:  0.001: Train: 0.7806 +- 0.0069, Test: 0.7221 +- 0.0348\n",
      "Degree:  5, Learning rate:  0.001, Lambda:   0.01: Train: 0.7806 +- 0.0050, Test: 0.7240 +- 0.0304\n",
      "Degree:  5, Learning rate:  0.001, Lambda:    0.1: Train: 0.7806 +- 0.0091, Test: 0.7173 +- 0.0203\n",
      "Degree:  5, Learning rate:  0.001, Lambda:    1.0: Train: 0.7761 +- 0.0070, Test: 0.7221 +- 0.0323\n",
      "Degree:  5, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7811 +- 0.0145, Test: 0.6827 +- 0.0596\n",
      "Degree:  5, Learning rate:   0.01, Lambda:  0.001: Train: 0.7793 +- 0.0160, Test: 0.6731 +- 0.0547\n",
      "Degree:  5, Learning rate:   0.01, Lambda:   0.01: Train: 0.7748 +- 0.0259, Test: 0.6721 +- 0.0576\n",
      "Degree:  5, Learning rate:   0.01, Lambda:    0.1: Train: 0.7639 +- 0.0257, Test: 0.6577 +- 0.0520\n",
      "Degree:  5, Learning rate:   0.01, Lambda:    1.0: Train: 0.7415 +- 0.0254, Test: 0.6913 +- 0.0290\n",
      "Degree:  5, Learning rate:    0.1, Lambda: 0.0001: Train: 0.7313 +- 0.0333, Test: 0.6240 +- 0.0506\n",
      "Degree:  5, Learning rate:    0.1, Lambda:  0.001: Train: 0.7157 +- 0.0258, Test: 0.6394 +- 0.0740\n",
      "Degree:  5, Learning rate:    0.1, Lambda:   0.01: Train: 0.7222 +- 0.0375, Test: 0.6413 +- 0.0473\n",
      "Degree:  5, Learning rate:    0.1, Lambda:    0.1: Train: 0.6748 +- 0.0336, Test: 0.6250 +- 0.0511\n",
      "Degree:  5, Learning rate:    0.1, Lambda:    1.0: Train: 0.6230 +- 0.0404, Test: 0.5981 +- 0.0523\n",
      "JET NUMBER 3:\n",
      "Degree:  1, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.6819 +- 0.0105, Test: 0.6256 +- 0.0602\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:  0.001: Train: 0.6850 +- 0.0115, Test: 0.6209 +- 0.0551\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:   0.01: Train: 0.6832 +- 0.0076, Test: 0.6302 +- 0.0574\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:    0.1: Train: 0.6796 +- 0.0076, Test: 0.6256 +- 0.0535\n",
      "Degree:  1, Learning rate: 0.0001, Lambda:    1.0: Train: 0.6809 +- 0.0095, Test: 0.6209 +- 0.0570\n",
      "Degree:  1, Learning rate:  0.001, Lambda: 0.0001: Train: 0.6904 +- 0.0132, Test: 0.6233 +- 0.0664\n",
      "Degree:  1, Learning rate:  0.001, Lambda:  0.001: Train: 0.6902 +- 0.0135, Test: 0.6302 +- 0.0592\n",
      "Degree:  1, Learning rate:  0.001, Lambda:   0.01: Train: 0.6972 +- 0.0145, Test: 0.6186 +- 0.0676\n",
      "Degree:  1, Learning rate:  0.001, Lambda:    0.1: Train: 0.6899 +- 0.0151, Test: 0.6233 +- 0.0357\n",
      "Degree:  1, Learning rate:  0.001, Lambda:    1.0: Train: 0.6855 +- 0.0135, Test: 0.6372 +- 0.0542\n",
      "Degree:  1, Learning rate:   0.01, Lambda: 0.0001: Train: 0.6948 +- 0.0202, Test: 0.6116 +- 0.0541\n",
      "Degree:  1, Learning rate:   0.01, Lambda:  0.001: Train: 0.7028 +- 0.0191, Test: 0.6465 +- 0.0664\n",
      "Degree:  1, Learning rate:   0.01, Lambda:   0.01: Train: 0.6946 +- 0.0161, Test: 0.6512 +- 0.0778\n",
      "Degree:  1, Learning rate:   0.01, Lambda:    0.1: Train: 0.6948 +- 0.0228, Test: 0.6186 +- 0.0552\n",
      "Degree:  1, Learning rate:   0.01, Lambda:    1.0: Train: 0.6592 +- 0.0390, Test: 0.6047 +- 0.0477\n",
      "Degree:  1, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6393 +- 0.0564, Test: 0.6256 +- 0.0583\n",
      "Degree:  1, Learning rate:    0.1, Lambda:  0.001: Train: 0.6320 +- 0.0376, Test: 0.5930 +- 0.0801\n",
      "Degree:  1, Learning rate:    0.1, Lambda:   0.01: Train: 0.6315 +- 0.0630, Test: 0.5977 +- 0.0806\n",
      "Degree:  1, Learning rate:    0.1, Lambda:    0.1: Train: 0.5904 +- 0.0479, Test: 0.5698 +- 0.0635\n",
      "Degree:  1, Learning rate:    0.1, Lambda:    1.0: Train: 0.5995 +- 0.0394, Test: 0.5512 +- 0.0682\n",
      "Degree:  2, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.6961 +- 0.0066, Test: 0.6488 +- 0.0514\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:  0.001: Train: 0.6917 +- 0.0087, Test: 0.6395 +- 0.0491\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:   0.01: Train: 0.6941 +- 0.0112, Test: 0.6488 +- 0.0514\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:    0.1: Train: 0.6948 +- 0.0098, Test: 0.6419 +- 0.0479\n",
      "Degree:  2, Learning rate: 0.0001, Lambda:    1.0: Train: 0.6930 +- 0.0119, Test: 0.6395 +- 0.0617\n",
      "Degree:  2, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7140 +- 0.0132, Test: 0.6395 +- 0.0609\n",
      "Degree:  2, Learning rate:  0.001, Lambda:  0.001: Train: 0.7168 +- 0.0231, Test: 0.6419 +- 0.0626\n",
      "Degree:  2, Learning rate:  0.001, Lambda:   0.01: Train: 0.7199 +- 0.0150, Test: 0.6209 +- 0.0361\n",
      "Degree:  2, Learning rate:  0.001, Lambda:    0.1: Train: 0.7114 +- 0.0144, Test: 0.6209 +- 0.0616\n",
      "Degree:  2, Learning rate:  0.001, Lambda:    1.0: Train: 0.7047 +- 0.0091, Test: 0.6186 +- 0.0512\n",
      "Degree:  2, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7258 +- 0.0265, Test: 0.6279 +- 0.0641\n",
      "Degree:  2, Learning rate:   0.01, Lambda:  0.001: Train: 0.7165 +- 0.0369, Test: 0.6023 +- 0.0709\n",
      "Degree:  2, Learning rate:   0.01, Lambda:   0.01: Train: 0.7147 +- 0.0306, Test: 0.6000 +- 0.0596\n",
      "Degree:  2, Learning rate:   0.01, Lambda:    0.1: Train: 0.7204 +- 0.0266, Test: 0.6302 +- 0.0535\n",
      "Degree:  2, Learning rate:   0.01, Lambda:    1.0: Train: 0.6747 +- 0.0400, Test: 0.6163 +- 0.0759\n",
      "Degree:  2, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6680 +- 0.0386, Test: 0.5953 +- 0.0840\n",
      "Degree:  2, Learning rate:    0.1, Lambda:  0.001: Train: 0.6566 +- 0.0440, Test: 0.6047 +- 0.0882\n",
      "Degree:  2, Learning rate:    0.1, Lambda:   0.01: Train: 0.6501 +- 0.0577, Test: 0.6000 +- 0.0784\n",
      "Degree:  2, Learning rate:    0.1, Lambda:    0.1: Train: 0.6362 +- 0.0477, Test: 0.5605 +- 0.1053\n",
      "Degree:  2, Learning rate:    0.1, Lambda:    1.0: Train: 0.5545 +- 0.0547, Test: 0.5442 +- 0.0966\n",
      "Degree:  3, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.6933 +- 0.0110, Test: 0.6442 +- 0.0521\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:  0.001: Train: 0.6946 +- 0.0111, Test: 0.6372 +- 0.0444\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:   0.01: Train: 0.6922 +- 0.0138, Test: 0.6395 +- 0.0444\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:    0.1: Train: 0.6938 +- 0.0090, Test: 0.6465 +- 0.0528\n",
      "Degree:  3, Learning rate: 0.0001, Lambda:    1.0: Train: 0.6953 +- 0.0094, Test: 0.6419 +- 0.0405\n",
      "Degree:  3, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7452 +- 0.0132, Test: 0.6465 +- 0.0558\n",
      "Degree:  3, Learning rate:  0.001, Lambda:  0.001: Train: 0.7362 +- 0.0200, Test: 0.6465 +- 0.0486\n",
      "Degree:  3, Learning rate:  0.001, Lambda:   0.01: Train: 0.7413 +- 0.0168, Test: 0.6512 +- 0.0649\n",
      "Degree:  3, Learning rate:  0.001, Lambda:    0.1: Train: 0.7370 +- 0.0153, Test: 0.6233 +- 0.0451\n",
      "Degree:  3, Learning rate:  0.001, Lambda:    1.0: Train: 0.7305 +- 0.0177, Test: 0.6233 +- 0.0413\n",
      "Degree:  3, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7486 +- 0.0143, Test: 0.6442 +- 0.0454\n",
      "Degree:  3, Learning rate:   0.01, Lambda:  0.001: Train: 0.7504 +- 0.0136, Test: 0.6349 +- 0.0521\n",
      "Degree:  3, Learning rate:   0.01, Lambda:   0.01: Train: 0.7336 +- 0.0216, Test: 0.6419 +- 0.0691\n",
      "Degree:  3, Learning rate:   0.01, Lambda:    0.1: Train: 0.7382 +- 0.0217, Test: 0.6395 +- 0.0512\n",
      "Degree:  3, Learning rate:   0.01, Lambda:    1.0: Train: 0.6961 +- 0.0285, Test: 0.6302 +- 0.0336\n",
      "Degree:  3, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6881 +- 0.0287, Test: 0.6186 +- 0.0532\n",
      "Degree:  3, Learning rate:    0.1, Lambda:  0.001: Train: 0.6995 +- 0.0348, Test: 0.6442 +- 0.0877\n",
      "Degree:  3, Learning rate:    0.1, Lambda:   0.01: Train: 0.6853 +- 0.0458, Test: 0.6023 +- 0.0564\n",
      "Degree:  3, Learning rate:    0.1, Lambda:    0.1: Train: 0.6558 +- 0.0369, Test: 0.6023 +- 0.0592\n",
      "Degree:  3, Learning rate:    0.1, Lambda:    1.0: Train: 0.5364 +- 0.0677, Test: 0.4977 +- 0.1041\n",
      "Degree:  4, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.6987 +- 0.0096, Test: 0.6372 +- 0.0392\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7010 +- 0.0114, Test: 0.6395 +- 0.0512\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7041 +- 0.0079, Test: 0.6465 +- 0.0439\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7021 +- 0.0096, Test: 0.6419 +- 0.0392\n",
      "Degree:  4, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7016 +- 0.0093, Test: 0.6372 +- 0.0490\n",
      "Degree:  4, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7594 +- 0.0177, Test: 0.6326 +- 0.0357\n",
      "Degree:  4, Learning rate:  0.001, Lambda:  0.001: Train: 0.7584 +- 0.0204, Test: 0.6442 +- 0.0442\n",
      "Degree:  4, Learning rate:  0.001, Lambda:   0.01: Train: 0.7550 +- 0.0204, Test: 0.6488 +- 0.0493\n",
      "Degree:  4, Learning rate:  0.001, Lambda:    0.1: Train: 0.7584 +- 0.0169, Test: 0.6465 +- 0.0538\n",
      "Degree:  4, Learning rate:  0.001, Lambda:    1.0: Train: 0.7450 +- 0.0138, Test: 0.6395 +- 0.0393\n",
      "Degree:  4, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7610 +- 0.0251, Test: 0.6512 +- 0.0488\n",
      "Degree:  4, Learning rate:   0.01, Lambda:  0.001: Train: 0.7499 +- 0.0339, Test: 0.6279 +- 0.0579\n",
      "Degree:  4, Learning rate:   0.01, Lambda:   0.01: Train: 0.7504 +- 0.0358, Test: 0.6372 +- 0.0467\n",
      "Degree:  4, Learning rate:   0.01, Lambda:    0.1: Train: 0.7375 +- 0.0378, Test: 0.6186 +- 0.0512\n",
      "Degree:  4, Learning rate:   0.01, Lambda:    1.0: Train: 0.7106 +- 0.0169, Test: 0.6465 +- 0.0664\n",
      "Degree:  4, Learning rate:    0.1, Lambda: 0.0001: Train: 0.6654 +- 0.0525, Test: 0.6209 +- 0.0658\n",
      "Degree:  4, Learning rate:    0.1, Lambda:  0.001: Train: 0.7008 +- 0.0597, Test: 0.6279 +- 0.0389\n",
      "Degree:  4, Learning rate:    0.1, Lambda:   0.01: Train: 0.6904 +- 0.0394, Test: 0.6302 +- 0.0482\n",
      "Degree:  4, Learning rate:    0.1, Lambda:    0.1: Train: 0.6344 +- 0.0564, Test: 0.6070 +- 0.0284\n",
      "Degree:  4, Learning rate:    0.1, Lambda:    1.0: Train: 0.5943 +- 0.0579, Test: 0.5814 +- 0.1030\n",
      "Degree:  5, Learning rate: 0.0001, Lambda: 0.0001: Train: 0.7067 +- 0.0108, Test: 0.6465 +- 0.0463\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:  0.001: Train: 0.7065 +- 0.0134, Test: 0.6488 +- 0.0422\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:   0.01: Train: 0.7098 +- 0.0129, Test: 0.6512 +- 0.0375\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:    0.1: Train: 0.7072 +- 0.0122, Test: 0.6442 +- 0.0390\n",
      "Degree:  5, Learning rate: 0.0001, Lambda:    1.0: Train: 0.7039 +- 0.0132, Test: 0.6535 +- 0.0381\n",
      "Degree:  5, Learning rate:  0.001, Lambda: 0.0001: Train: 0.7729 +- 0.0138, Test: 0.6605 +- 0.0552\n",
      "Degree:  5, Learning rate:  0.001, Lambda:  0.001: Train: 0.7664 +- 0.0085, Test: 0.6581 +- 0.0510\n",
      "Degree:  5, Learning rate:  0.001, Lambda:   0.01: Train: 0.7693 +- 0.0118, Test: 0.6558 +- 0.0518\n",
      "Degree:  5, Learning rate:  0.001, Lambda:    0.1: Train: 0.7682 +- 0.0143, Test: 0.6488 +- 0.0545\n",
      "Degree:  5, Learning rate:  0.001, Lambda:    1.0: Train: 0.7628 +- 0.0139, Test: 0.6349 +- 0.0313\n",
      "Degree:  5, Learning rate:   0.01, Lambda: 0.0001: Train: 0.7597 +- 0.0241, Test: 0.6558 +- 0.0507\n",
      "Degree:  5, Learning rate:   0.01, Lambda:  0.001: Train: 0.7711 +- 0.0149, Test: 0.6581 +- 0.0295\n",
      "Degree:  5, Learning rate:   0.01, Lambda:   0.01: Train: 0.7607 +- 0.0215, Test: 0.6605 +- 0.0479\n",
      "Degree:  5, Learning rate:   0.01, Lambda:    0.1: Train: 0.7610 +- 0.0239, Test: 0.6419 +- 0.0542\n",
      "Degree:  5, Learning rate:   0.01, Lambda:    1.0: Train: 0.7212 +- 0.0238, Test: 0.6395 +- 0.0626\n",
      "Degree:  5, Learning rate:    0.1, Lambda: 0.0001: Train: 0.7145 +- 0.0456, Test: 0.6488 +- 0.0545\n",
      "Degree:  5, Learning rate:    0.1, Lambda:  0.001: Train: 0.7067 +- 0.0288, Test: 0.6395 +- 0.0553\n",
      "Degree:  5, Learning rate:    0.1, Lambda:   0.01: Train: 0.7137 +- 0.0397, Test: 0.6465 +- 0.0614\n",
      "Degree:  5, Learning rate:    0.1, Lambda:    0.1: Train: 0.6703 +- 0.0363, Test: 0.6093 +- 0.0905\n",
      "Degree:  5, Learning rate:    0.1, Lambda:    1.0: Train: 0.5801 +- 0.0617, Test: 0.5791 +- 0.0835\n"
     ]
    }
   ],
   "source": [
    "data_splits_train = split_data_by_feature(y_train, X_train, ids, feature_ids[\"PRI_jet_num\"], train=True)\n",
    "\n",
    "# Define hyperparameters\n",
    "seed = 44\n",
    "k_fold = 10\n",
    "\n",
    "max_iters = 300\n",
    "batch_size = 100\n",
    "\n",
    "# MAYBE RANGES SHOULD BE SELECTED BETTER???\n",
    "gammas = np.logspace(-4, -1, 4)     # learning rate   \n",
    "degrees = [1, 2, 3, 4, 5]           # polynomial expansion degree\n",
    "lambdas = np.logspace(-4, 0, 5)     # regularization constant\n",
    "\n",
    "accs_train = {}\n",
    "accs_test = {}\n",
    "\n",
    "\n",
    "# perform k-fold cross validation for each dataset (jet) separately\n",
    "for jet in data_splits_train.keys():\n",
    "    X_train_jet, y_train_jet, _ = data_splits_train[jet]\n",
    "    \n",
    "    print(f\"JET NUMBER {int(jet)}:\")\n",
    "    \n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train_jet, k_fold, seed)\n",
    "\n",
    "    # define lists to store the accuracies of training data and test data\n",
    "    accs_train_jet = np.zeros((len(degrees), len(gammas), len(lambdas)))\n",
    "    accs_test_jet = np.zeros((len(degrees), len(gammas), len(lambdas)))\n",
    "\n",
    "    for id_degree, degree in enumerate(degrees):\n",
    "        for id_gamma, gamma in enumerate(gammas):\n",
    "            for id_lambda, lambda_ in enumerate(lambdas):\n",
    "                cur_acc_train = np.zeros(k_fold)\n",
    "                cur_acc_test = np.zeros(k_fold)\n",
    "\n",
    "                for k in range(k_fold):\n",
    "                    acc_train, acc_test = cross_validation(y=y_train_jet, X=X_train_jet, k_indices=k_indices, k=k, \n",
    "                                                           degree=degree, gamma=gamma, lambda_=lambda_, \n",
    "                                                           max_iters=max_iters, batch_size=batch_size)\n",
    "\n",
    "                    cur_acc_train[k] = acc_train\n",
    "                    cur_acc_test[k] = acc_test\n",
    "\n",
    "                accs_train_jet[id_degree, id_gamma, id_lambda] = cur_acc_train.mean()\n",
    "                accs_test_jet[id_degree, id_gamma, id_lambda] = cur_acc_test.mean()\n",
    "                print(f\"Degree: {degree:2}, Learning rate: {gamma:6}, Lambda: {lambda_:6}: Train: {cur_acc_train.mean():.4f} +- {cur_acc_train.std():.4f}, Test: {cur_acc_test.mean():.4f} +- {cur_acc_test.std():.4f}\")\n",
    "\n",
    "    accs_train[jet] = accs_train_jet\n",
    "    accs_test[jet] = accs_test_jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JET 0.0: Train accuracy: 0.7375\n",
      "JET 1.0: Train accuracy: 0.6977\n",
      "JET 2.0: Train accuracy: 0.7514\n",
      "JET 3.0: Train accuracy: 0.6597\n",
      "TOGETHER: Train accuracy on train data: 0.7211\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'data/train.csv'\n",
    "y_train_whole, X_train_whole, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)\n",
    "\n",
    "data_splits_train_whole = split_data_by_feature(y_train_whole, X_train_whole, ids, feature_ids[\"PRI_jet_num\"], train=True)\n",
    "\n",
    "weights = {}\n",
    "best_degrees = {}\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "removed_features = {}\n",
    "for jet in data_splits_train_whole.keys():\n",
    "    X_train_whole_jet, y_train_whole_jet, _ = data_splits_train_whole[jet]\n",
    "    \n",
    "    id_degree, id_gamma, id_lambda = np.unravel_index(np.argmax(accs_test[jet]), accs_test[jet].shape)\n",
    "    degree, gamma, lambda_ = degrees[id_degree], gammas[id_gamma], lambdas[id_lambda]\n",
    "    best_degrees[jet] = degree\n",
    "    \n",
    "    X_train_whole_jet, removed_features_jet = remove_tiny_features(X_train_whole_jet)\n",
    "    removed_features[jet] = removed_features_jet\n",
    "    \n",
    "    # remove outliers\n",
    "    # impute values when they are missing (value -999 I think) (e.g. by median)\n",
    "    \n",
    "    X_train_whole_jet = build_poly(X_train_whole_jet, degree)\n",
    "    X_train_whole_jet = standardize(X_train_whole_jet)\n",
    "\n",
    "    w0 = np.zeros(X_train_whole_jet.shape[1])\n",
    "    w, loss = reg_logistic_regression(y=y_train_whole_jet, tx=X_train_whole_jet, lambda_=lambda_, initial_w=w0, max_iters=max_iters, gamma=gamma, batch_size=batch_size)\n",
    "    weights[jet] = w\n",
    "        \n",
    "    y_train_pred = predict_labels(w, X_train_whole_jet)\n",
    "    acc_train = accuracy(y_train_pred, y_train_whole_jet)\n",
    "    print(f\"JET {jet}: Train accuracy: {acc_train:.4f}\")\n",
    "        \n",
    "    all_preds.append(y_train_pred)\n",
    "    all_labels.append(y_train_whole_jet)\n",
    "\n",
    "    \n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "acc_train_whole = accuracy(all_preds, all_labels)\n",
    "print(f\"TOGETHER: Train accuracy on train data: {acc_train_whole:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = './data/test.csv'\n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "data_splits_test = split_data_by_feature(None, X_test, ids_test, feature_ids[\"PRI_jet_num\"], train=False)\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "all_ids = []\n",
    "for jet in data_splits_test.keys():\n",
    "    X_test_jet, _, ids_jet = data_splits_test[jet]\n",
    "    \n",
    "    X_test_jet = remove_custom_features(X_test_jet, custom_feature_ids=removed_features[jet])\n",
    "    \n",
    "    # remove outliers\n",
    "    # impute values when they are missing (value -999 I think) (e.g. by median)\n",
    "    \n",
    "    X_test_jet = build_poly(X_test_jet, best_degrees[jet])\n",
    "    X_test_jet = standardize(X_test_jet)\n",
    "    \n",
    "    y_test_pred = predict_labels(weights[jet], X_test_jet, competition=True)\n",
    "    \n",
    "    all_preds.append(y_test_pred)\n",
    "    all_ids.append(ids_jet)\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_ids = np.concatenate(all_ids, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = './predictions/predictions.csv'\n",
    "create_csv_submission(all_ids, all_preds, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
